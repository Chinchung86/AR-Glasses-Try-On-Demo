<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <title>Unified AR Glasses Try-On</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <!-- A-Frame library -->
    <script src="https://aframe.io/releases/1.3.0/aframe.min.js"></script>
    <!-- Mediapipe FaceMesh and helpers -->
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js"></script>
    <!-- QR Code generation library -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/qrcodejs/1.0.0/qrcode.min.js"></script>
    <style>
      body { font-family: sans-serif; text-align: center; margin: 0; padding: 0; }
      /* Container for A-Frame scene */
      #modelViewerContainer { width: 100%; max-width: 600px; margin: 20px auto; }
      /* Desktop UI elements */
      #desktopUI { display: none; }
      /* Mobile UI elements */
      #mobileUI { display: none; }
      /* QR code container styling */
      #qrContainer { margin-top: 20px; }
      button { margin: 10px; padding: 10px 20px; font-size: 16px; }
      /* Video element for AR face tracking */
      #webcam { width: 640px; height: 480px; border: 1px solid #ccc; margin: 10px auto; display: none; }
    </style>
  </head>
  <body>
    <!-- A-Frame Model Viewer Section (same for both desktop and mobile) -->
    <div id="modelViewerContainer">
      <a-scene embedded>
        <a-assets>
          <a-asset-item id="glassesModel" src="https://raw.githubusercontent.com/Chinchung86/AR-Glasses-Try-On-Demo/main/glasses.glb"></a-asset-item>
        </a-assets>
        <!-- This entity shows the glasses model. Initial position and scale may require calibration. -->
        <a-entity id="glasses" gltf-model="#glassesModel" position="0 1.6 -2" scale="0.5 0.5 0.5"></a-entity>
        <a-camera position="0 1.6 0"></a-camera>
        <a-light type="ambient" color="#FFF"></a-light>
      </a-scene>
    </div>

    <!-- Desktop UI Section -->
    <div id="desktopUI">
      <!-- QR Code Section -->
      <div id="qrContainer"></div>
      <!-- Desktop Try On Button -->
      <button id="tryOnDesktopBtn">Start AR Try-On (Desktop)</button>
    </div>

    <!-- Mobile UI Section -->
    <div id="mobileUI">
      <!-- Mobile Try On Button -->
      <button id="tryOnMobileBtn">Start AR Try-On (Mobile)</button>
    </div>

    <!-- Video element for webcam capture (used in both modes) -->
    <video id="webcam" autoplay playsinline></video>

    <script>
      // Basic mobile detection (you can refine this)
      const isMobile = /Mobi|Android/i.test(navigator.userAgent);

      // Show either the mobile or desktop UI based on the detection.
      if (isMobile) {
        document.getElementById("mobileUI").style.display = "block";
      } else {
        document.getElementById("desktopUI").style.display = "block";
      }
      
      // --- QR CODE GENERATION (only display on desktop) ---
      if (!isMobile) {
        // The generated QR code can point to the same URL of this page.
        // In a real scenario, you might have a dedicated mobile URL.
        new QRCode(document.getElementById("qrContainer"), {
          text: window.location.href,
          width: 128,
          height: 128
        });
      }
      
      // --- MEDIAPIPE SETUP FOR FACE TRACKING ---
      const videoElement = document.getElementById("webcam");
      const glassesEntity = document.getElementById("glasses");

      const faceMesh = new FaceMesh({
        locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`
      });

      faceMesh.setOptions({
        maxNumFaces: 1,
        refineLandmarks: true,
        minDetectionConfidence: 0.5,
        minTrackingConfidence: 0.5
      });

      faceMesh.onResults(onResults);

      function onResults(results) {
        if (results.multiFaceLandmarks && results.multiFaceLandmarks.length > 0) {
          const landmarks = results.multiFaceLandmarks[0];
          // For instance, we use landmark 6 as a reference (adjust this if needed).
          const noseTip = landmarks[6];
          // Mapping from normalized Mediapipe coordinates ([0,1]) to A-Frame world coordinates.
          const newPos = {
            x: (noseTip.x - 0.5) * 2,
            y: (0.5 - noseTip.y) * 2 + 1.6,
            z: -2
          };
          glassesEntity.setAttribute("position", newPos);
        }
      }

      // Create a function to initialize the camera with correct constraints.
      function startCamera() {
        // For mobile, explicitly request the front camera; desktop can use the default.
        const cameraConstraints = {
          width: 640,
          height: 480,
          facingMode: isMobile ? "user" : "environment"
        };

        const camera = new Camera(videoElement, {
          onFrame: async () => {
            await faceMesh.send({ image: videoElement });
          },
          ...cameraConstraints
        });
        
        // Make sure the video element is visible once the camera starts.
        videoElement.style.display = 'block';
        camera.start();
      }

      // Event listeners for the try-on buttons.
      if (!isMobile) {
        document.getElementById("tryOnDesktopBtn").addEventListener("click", startCamera);
      } else {
        document.getElementById("tryOnMobileBtn").addEventListener("click", startCamera);
      }
    </script>
  </body>
</html>
